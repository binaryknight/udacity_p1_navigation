{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* The code in this repository implements the [Double DQN algorithm](https://arxiv.org/abs/1509.06461)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Function Approximation\n",
    "* The Q-function update in standard Q learning is\n",
    "\\begin{align}\n",
    "Q(s_t,a_t) &\\leftarrow Q(s_t,a_t) + \\alpha(r_{t+1} + \\gamma\\max_{a}Q(s_{t+1},a) - Q(s_t,a_t))\\\\\n",
    "\\Delta(s_t, a_t) &= \\alpha(r_{t+1} + \\gamma\\max_{a}Q(s_{t+1},a) - Q(s_t,a_t))\n",
    "\\end{align}\n",
    "where\n",
    "   * $s_t$ is the state at time $t$,\n",
    "   * $a_t$ is the action taken at time $t$,\n",
    "   * $r_{t+1}$ is the reward obtained, \n",
    "   * $s_{t+1}$ is the state at $t+1$, \n",
    "   * $\\gamma \\in (0,1]$ is the discount factor\n",
    "   * $\\alpha \\in (0,1]$ is the step size.\n",
    "\n",
    "\n",
    "* In the  DQN (Deep Q-network) algorithm, Q values are estimated using neural networks, and the update rule becomes \n",
    "  \\begin{align}\n",
    "   \\Delta(s_t, a_t, w) &= \\alpha(r_{t+1} + \\gamma\\max_{a}\\hat{Q}(s_{t+1},a,w) - \\hat{Q}(s_t,a_t,w))\n",
    "  \\end{align}\n",
    "  where \n",
    "    * $\\hat{Q}$ is the neural network approximation to the Q-function called the local neural network, and\n",
    "    * $w$ are the weights of the local neural network.\n",
    "\n",
    "* DQN makes the following modification in order to train the local neural network in a stable manner:\n",
    "  \\begin{align}\n",
    "    \\Delta(s_t, a_t, w, \\bar{w}) &= \\alpha(r_{t+1} + \\gamma\\max_{a}\\hat{Q}(s_{t+1},a,\\bar{w}) - \\hat{Q}(s_t,a_t,w))\n",
    "  \\end{align}\n",
    "  where \n",
    "   * $\\bar{w}$ are the weights of the target neural network. \n",
    "\n",
    "* The $\\bar{w}$ are obtained as follows:\n",
    "  \\begin{align}   \n",
    "     \\bar{w}(t) &= w(n\\cdot T)\\;\\; \\forall t\\; \\in  \\lbrack n \\cdot T,(n+1)\\cdot T)  \n",
    "  \\end{align}  \n",
    "  where \n",
    "   * $n$ is a non-negative integer,  \n",
    "   * $T$ is the update period an is a positive integer.\n",
    "\n",
    "* Instead of keeping the target neural network constant for a duration $T$, one can update the target neural network at every update of the local neural network as follows:\n",
    "\\begin{align}\n",
    "\\bar{w} &\\leftarrow \\tau w  + (1 -\\tau) \\bar{w} \n",
    "\\end{align}\n",
    "where $\\tau \\in [0,1]$.  The weighted sum allows $\\bar{w}$ to change relatively more slowly  for values of $\\tau$ closer to $0$.\n",
    "\n",
    "* The double DQN algorithm tries to prevent the overestimation of the Q-function using both the target and local network weights in computing the next state's Q-function value;\n",
    "  \\begin{align}\n",
    "    \\Delta(s_t, a_t, w, \\bar{w}) &= \\alpha(r_{t+1} + \\gamma \\hat{Q}(s_{t+1},\\mathop{argmax}_a \\hat{Q}(s_{t+1}, a, w),\\bar{w}) - \\hat{Q}(s_t,a_t,w))\n",
    "  \\end{align} where  $r_{t+1} + \\gamma \\hat{Q}(s_{t+1},\\mathop{argmax}_a \\hat{Q}(s_{t+1}, a, w),\\bar{w})$ is the TD target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Neural Network Training\n",
    "* In order to train the target neural network, states and rewards are used.\n",
    "* At  every time step $t$, the following quantities are stored for training;\n",
    "    * $s_t$: the current state,\n",
    "    * $a_t$: the current action,\n",
    "    * $r_{t+1}$: the reward of the current action\n",
    "    * $s_{t+1}$: the next state.\n",
    "\n",
    "\n",
    "\n",
    "* The set $\\{s_t, a_t, r_{t+1}, s_{t+1}\\}$ is called an experience.\n",
    "* Experiences are saved in a buffer called the experience replay buffer.\n",
    "* The experiences can be stored in a circular buffer of size $B$ where $B \\geq N$. \n",
    "* At every training step, a batch of $N$ experiences are randomly selected to update the local network weights.\n",
    "* The local neural network is trained to minimize the root mean squared difference between the TD target and the estimate $\\hat{Q}(s_t,a_t,w)$.\n",
    "* The local neural network weights are updated every $T$ time steps:\n",
    "\\begin{align}\n",
    "  y_i    &= r_{t+1, i} + \\gamma \\hat{Q}(s_{t+1,i},\\mathop{argmax}_a \\hat{Q}(s_{t+1,i}, a, w),\\bar{w})\\;\\; \\forall i \\in \\{1, \\dots, N\\}\\\\\n",
    "  \\hat{y}_i & = \\hat{Q}(s_{t,i}, a_{t,i}, w)\\;\\; \\forall; i \\in \\{1, \\dots, N\\} \\\\\n",
    "\\Delta w &= \\sum_{i=1}^{N}\\alpha( y_i - \\hat{y}_i) \\nabla  \\hat{Q}_w(s_{t,i}, a_{t,i}, w)\\\\\n",
    "w &\\leftarrow w + \\Delta w\n",
    "\\end{align}\n",
    "* The target neural network weights are updated using\n",
    "\\begin{align}\n",
    "\\bar{w} &\\leftarrow \\tau w  + (1 -\\tau) \\bar{w} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training Algorithm\n",
    "\n",
    "1. Set the following hyper parameters:\n",
    "   * $\\epsilon$: the frequency of random action choices, $\\epsilon \\in [0,1]$;\n",
    "   * $\\gamma_\\epsilon$: decay rate for $\\epsilon$, $\\gamma_\\epsilon \\in [0,1]$;\n",
    "   * $\\epsilon_{min}$: lower bound on $\\epsilon$\n",
    "   * $\\alpha$: the learning rate, $\\alpha \\in (0,1]$;\n",
    "   * $\\gamma$: the discount factor, $\\gamma \\in (0,1]$;\n",
    "   * $\\tau$: target weight update factor, $\\tau in (0,1]$;\n",
    "   * $T$: period of local network update, $T > 0$;\n",
    "   * $N$: training batch size, $N > 0$;\n",
    "   * $B$: circular buffer capacity, $B \\leq >N$.\n",
    "   * $N_e$: set the maximum number of episodes used in training.\n",
    "   \n",
    "2. Determine the local and target neural network structure. \n",
    "   1.  Initialize $w$ and $\\bar{w}$, $\\bar{w} = w$.\n",
    "3. Perform training:\n",
    "   1. Start new episode while number of episodes is less than $N_e$\n",
    "      1. Choose action $a_t$ using  $\\hat{Q}$ and the $\\epsilon$-Greedy algorithm.\n",
    "      2. Take the action $a_t$, observe the reward $r_{t+1}$, transition to next state $s_{t+1}$. \n",
    "      3. Store the experience $\\{s_t, a_t, r_{t+1}, s_{t+1}\\}$ in the circular  buffer\n",
    "      4. If $t = nT$ for some positive integer $n$ then update the target and local neural network weights as follows\n",
    "          1. Get $N$ random experiences from the replay buffer. \n",
    "          2. Update the local network weight $w$:\n",
    "             \\begin{align}\n",
    "                y_i    &= r_{t+1, i} + \\gamma \\hat{Q}(s_{t+1,i},\\mathop{argmax}_a \\hat{Q}(s_{t+1,i}, a, w),\\bar{w})\\;\\;  \\forall i \\in \\{1, \\dots, N\\}\\\\\n",
    "             \\hat{y}_i & = \\hat{Q}(s_{t,i}, a_{t,i}, w)\\;\\; \\forall i \\in \\{1,\\dots,N\\} \\\\\n",
    "             \\Delta w &= \\sum_{i=1}^{N}\\alpha( y_i - \\hat{y}_i) \\nabla  \\hat{Q}_w(s_{t,i}, a_{t,i}, w)\\\\\n",
    "              w &\\leftarrow w + \\Delta w\n",
    "             \\end{align}\n",
    "          3. Update the target network weight\n",
    "             \\begin{align}\n",
    "                \\bar{w} &\\leftarrow \\tau w  + (1 -\\tau) \\bar{w} \n",
    "            \\end{align}\n",
    "      5. Update $\\epsilon$:\n",
    "         \\begin{align*}\n",
    "           \\epsilon &\\leftarrow \\max(\\epsilon_{min}, \\gamma_{epsilon}\\epsilon)\n",
    "         \\end{align*}\n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "* The hyperparameters used in the training are:\n",
    "   * $\\epsilon=1.0$\n",
    "   * $\\gamma_\\epsilon=0.8$\n",
    "   * $\\epsilon_{min}= 0.0001$\n",
    "   * $\\alpha=0.001$\n",
    "   * $\\gamma=0.99$\n",
    "   * $\\tau = 0.01$\n",
    "   * $T =4$\n",
    "   * $N = 256$ \n",
    "   * $B = 10000$.\n",
    "   * $N_e = 1800$.\n",
    "\n",
    "* The hyperparameters are determined by trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and Target Neural Network Structure\n",
    "* The structure of the local and target neural networks are the same.\n",
    "    * The input layers have 37 nodes each corresponding to the dimension of the signal received by the environment.\n",
    "    * The outputs of the input layer nodes  are fed to RELU activation functions.\n",
    "    * Each network has 2 hidden layers with 64 nodes in each layer.\n",
    "    * The hidden layer nodes' outputs are fed to RELU activation functions.\n",
    "    * The output layers have 4 nodes each corresponding to the number of available actions. \n",
    "* The initial weights $w$ consists of random numbers.\n",
    "* The Adam optimization algorithm is used to determine $\\Delta w$. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results\n",
    "* The agent achieves an average score of 13 or above in about 360 episodes.\n",
    "* The progression of the scores and the average score is shown in the next figure.\n",
    "![Training Performance](training_performance.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Enhancements\n",
    "* The hyper parameters for the algorithm\thave been chosen by trial and error. Better parameters can be found by using an\tautomated hyperparameter search\ttechniques.\n",
    "* Intuitively,\t increasing the number of training\tsamples\tshould improve the time performance, but would it woul also increase the training time. [Prioritized experienced replay](https://arxiv.org/abs/1511.05952) can improve the performance with less\tincrease in training time. Here, the training samples are given priorities based on\ttheir TD error.\n",
    "\n",
    "* Finally, there are many improvements to the vanilla DQN algorithm. One can combine all into to obtain\tthe [Rainbow algorithm](https://arxiv.org/abs/1710.02298).\n",
    "* One open question is how to reduce the variance in the scores. The average score is improved as a result of training, but the scores vary a lot around this average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
